# This schedule follows the methodology proposed by Intel Labs China in the paper:
#   Dynamic Network Surgery for Efficient DNNs, Yiwen Guo, Anbang Yao, Yurong Chen.
#   NIPS 2016, https://arxiv.org/abs/1600.504493.
#
# We have not been able to see great results with our implementation of the paper, as we understood it.  In the paper
# two sets of weights are used: in the forward-pass Guo et. al use masked weights to compute the loss, but in the
# backward-pass they update the unmasked weights (using gradients computed from the masked-weights loss).
# To replicate this behavior set in the SplicingPruner policy:
#   use_double_copies: True
#   mask_on_forward_only: True
#
# We found that using two copies of weights reduces the accuracy results, and so we disable this configuration in the
# example schedule below.
#
# The "mask_on_forward_only" configuration controls what we do after the weights are updated by the backward pass.
# In issue #53 (https://github.com/NervanaSystems/distiller/issues/53) we explain why in some cases masked weights
# will be updated to a non-zero value, even if their gradients are masked (e.g. when using SGD with momentum).
# Therefore, to circumvent this weights-update performed by the backward pass, we usually mask the weights again -
# right after the backward pass.  To disable this masking set:
#   mask_on_forward_only: False
#
#
# Baseline results:
#     Top1: 91.780    Top5: 99.710    Loss: 0.376
#     Total MACs: 40,813,184
#     # of parameters: 270,896
#
# Results:
#     Top1: 91.48
#     Total MACs: 40,813,184
#     Total sparsity: 64.3%
#     # of parameters: 97,176
#
# time python3 compress_classifier.py --arch resnet20_cifar  ../../../data.cifar10 -p=50 --lr=0.01 --epochs=180 --compress=../network_surgery/resnet20.network_surgery.yaml -j=1 --deterministic  --validation-size=0 --resume=../ssl/checkpoints/checkpoint_trained_dense.pth.tar --masks-sparsity --num-best-scores=10
#
# Parameters:
# +----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
# |    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
# |----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
# |  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            432 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.36214 | -0.00708 |    0.25825 |
# |  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            590 |    0.00000 |    0.00000 |  6.25000 | 23.43750 |  0.00000 |   74.39236 | 0.12122 | -0.00725 |    0.05601 |
# |  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            565 |    0.00000 |    0.00000 |  0.00000 | 23.43750 |  0.00000 |   75.47743 | 0.11794 | -0.00092 |    0.05356 |
# |  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            695 |    0.00000 |    0.00000 |  0.00000 | 14.45312 |  0.00000 |   69.83507 | 0.10397 | -0.00927 |    0.05366 |
# |  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            661 |    0.00000 |    0.00000 |  0.00000 | 13.67188 |  0.00000 |   71.31076 | 0.09704 | -0.00361 |    0.04882 |
# |  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |           1016 |    0.00000 |    0.00000 |  0.00000 |  7.03125 |  0.00000 |   55.90278 | 0.14338 | -0.00710 |    0.08502 |
# |  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            615 |    0.00000 |    0.00000 |  0.00000 | 24.60938 |  0.00000 |   73.30729 | 0.10684 |  0.00189 |    0.05180 |
# |  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2196 |    0.00000 |    0.00000 |  0.00000 |  4.68750 |  0.00000 |   52.34375 | 0.11522 |  0.00071 |    0.07299 |
# |  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           3694 |    0.00000 |    0.00000 |  0.00000 |  4.39453 |  0.00000 |   59.91753 | 0.09577 | -0.00448 |    0.05684 |
# |  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            120 |    0.00000 |    0.00000 |  0.00000 | 76.56250 |  6.25000 |   76.56250 | 0.20066 | -0.00326 |    0.08835 |
# | 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2923 |    0.00000 |    0.00000 |  0.00000 |  7.42188 |  0.00000 |   68.28342 | 0.08086 | -0.00464 |    0.04331 |
# | 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2895 |    0.00000 |    0.00000 |  0.00000 |  7.03125 |  0.00000 |   68.58724 | 0.07065 | -0.00295 |    0.03783 |
# | 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3748 |    0.00000 |    0.00000 |  0.00000 |  3.71094 |  0.00000 |   59.33160 | 0.08423 | -0.00755 |    0.05010 |
# | 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2718 |    0.00000 |    0.00000 |  0.00000 | 12.40234 |  0.00000 |   70.50781 | 0.06441 |  0.00110 |    0.03343 |
# | 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           7476 |    0.00000 |    0.00000 |  0.00000 |  6.49414 |  0.00000 |   59.44010 | 0.08217 | -0.00281 |    0.04937 |
# | 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          18887 |    0.00000 |    0.00000 |  0.00000 |  0.29297 |  0.00000 |   48.76573 | 0.07901 | -0.00271 |    0.05232 |
# | 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            668 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.11194 | -0.00590 |    0.06115 |
# | 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          11653 |    0.00000 |    0.00000 |  0.00000 |  5.54199 |  0.00000 |   68.38921 | 0.07055 | -0.00359 |    0.03791 |
# | 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11657 |    0.00000 |    0.00000 |  0.00000 |  6.56738 |  0.00000 |   68.37836 | 0.06273 | -0.00401 |    0.03371 |
# | 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          12120 |    0.00000 |    0.00000 |  0.00000 |  7.83691 |  0.00000 |   67.12240 | 0.06262 | -0.00510 |    0.03428 |
# | 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |          11207 |    0.00000 |    0.00000 |  0.00000 | 27.53906 |  0.00000 |   69.59907 | 0.03841 |  0.00013 |    0.02018 |
# | 21 | module.fc.weight                    | (10, 64)       |           640 |            640 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.56258 | -0.00002 |    0.48535 |
# | 22 | Total sparsity:                     | -              |        270896 |          97176 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.12793 | 0.00000 |  0.00000 |    0.00000 |
# +----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
# Total sparsity: 64.13
#
# --- validate (epoch=359)-----------
# 10000 samples (256 per mini-batch)
# ==> Top1: 91.480    Top5: 99.600    Loss: 0.363
#
# ==> Best Top1: 91.790 (0.0 sparsity) on Epoch: 181
#
# Saving checkpoint to: logs/2018.10.31-232827/checkpoint.pth.tar
# --- test ---------------------
# 10000 samples (256 per mini-batch)
# ==> Top1: 91.480    Top5: 99.600    Loss: 0.363
#
#
# Log file for this run: /home/cvds_lab/nzmora/pytorch_workspace/distiller/examples/classifier_compression/logs/2018.10.31-232827/2018.10.31-232827.log
#
# real    64m27.317s
# user    118m46.020s
# sys     14m3.627s

version: 1
pruners:
  pruner1:
    class: SplicingPruner
    low_thresh_mult: 0.9
    hi_thresh_mult: 1.1
    sensitivity_multiplier: 0.015
    sensitivities:
      #'module.conv1.weight': 0.50
      module.layer1.0.conv1.weight: 0.50
      module.layer1.0.conv2.weight: 0.50
      module.layer1.1.conv1.weight: 0.50
      module.layer1.1.conv2.weight: 0.50
      module.layer1.2.conv1.weight: 0.30
      module.layer1.2.conv2.weight: 0.50
      module.layer2.0.conv1.weight: 0.30
      module.layer2.0.conv2.weight: 0.40
      module.layer2.0.downsample.0.weight: 0.50
      module.layer2.1.conv1.weight: 0.50
      module.layer2.1.conv2.weight: 0.50
      module.layer2.2.conv1.weight: 0.40
      module.layer2.2.conv2.weight: 0.50
      module.layer3.0.conv1.weight: 0.40
      module.layer3.0.conv2.weight: 0.30
      module.layer3.0.downsample.0.weight: 0.50
      module.layer3.1.conv1.weight: 0.50
      module.layer3.1.conv2.weight: 0.50
      module.layer3.2.conv1.weight: 0.50
      module.layer3.2.conv2.weight: 0.50
      #module.fc.weight

lr_schedulers:
  training_lr:
    class: StepLR
    step_size: 45
    gamma: 0.10

policies:
  - pruner:
      instance_name: pruner1
      args:
        keep_mask: True
        mini_batch_pruning_frequency: 1
        mask_on_forward_only: True
        #use_double_copies: True
    starting_epoch: 180
    ending_epoch: 280
    frequency: 1


  - lr_scheduler:
      instance_name: training_lr
    starting_epoch: 225
    ending_epoch: 400
    frequency: 1
